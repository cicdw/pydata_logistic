{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some example implementations of Newton's Method for Logisitic Regression\n",
    "\n",
    "<a href=\"#params_setup\">Parameter Setup</a><br>\n",
    "<a href=\"#algorithms\">Algorithms</a><br>\n",
    "<a href=\"#newton\">Newton's Method</a><br>\n",
    "<a href=\"#conv\">Convergence Criteria</a><br>\n",
    "<a href=\"#numerics\">Numerical Examples</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import warnings\n",
    "\n",
    "# import statsmodels.formula.api as smf # if you want to check results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data_setup\"></a>\n",
    "## Setup\n",
    "\n",
    "### Parameter / Data Setup\n",
    "\n",
    "In the below cells, there are various parameters and options to play with involving data creation, algorithm settings, and what model you want to try and fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def log_likelihood(p, y):\n",
    "    '''Computes the log-likelihood'''\n",
    "    \n",
    "    return np.sum(y*np.log(p) + (1-y)*np.log(1-p)).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20009) # set the seed\n",
    "\n",
    "## algorithm settings\n",
    "tol=1e-8 # convergence tolerance\n",
    "lam = None # l2-regularization\n",
    "max_iter = 20 # maximum allowed iterations\n",
    "\n",
    "## data creation settings\n",
    "r = 0.95 # covariance between x and z\n",
    "n = 1000 # number of observations\n",
    "sigma = 1 # variance of noise\n",
    "\n",
    "## model settings\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 # true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 # variances of inputs\n",
    "\n",
    "## the model specification you want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>v</th>\n",
       "      <th>np.exp(x)</th>\n",
       "      <th>I(v ** 2 + z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510605</td>\n",
       "      <td>0.362093</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>1.666299</td>\n",
       "      <td>0.362104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.119423</td>\n",
       "      <td>-0.318148</td>\n",
       "      <td>-151.289989</td>\n",
       "      <td>0.887432</td>\n",
       "      <td>22888.342554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.227797</td>\n",
       "      <td>1.144017</td>\n",
       "      <td>-17.183399</td>\n",
       "      <td>3.413701</td>\n",
       "      <td>296.413213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.439006</td>\n",
       "      <td>0.577195</td>\n",
       "      <td>-123.230617</td>\n",
       "      <td>1.551165</td>\n",
       "      <td>15186.362205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.008351</td>\n",
       "      <td>-0.287105</td>\n",
       "      <td>820.477948</td>\n",
       "      <td>0.991684</td>\n",
       "      <td>673183.776118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n",
       "0        1.0  0.510605  0.362093   -0.003417   1.666299       0.362104\n",
       "1        1.0 -0.119423 -0.318148 -151.289989   0.887432   22888.342554\n",
       "2        1.0  1.227797  1.144017  -17.183399   3.413701     296.413213\n",
       "3        1.0  0.439006  0.577195 -123.230617   1.551165   15186.362205\n",
       "4        1.0 -0.008351 -0.287105  820.477948   0.991684  673183.776118"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the data\n",
    "\n",
    "x, z = np.random.multivariate_normal([0,0], [[var_x,r],[r,var_z]], n).T\n",
    "v = np.random.normal(0,var_v,n)**3\n",
    "\n",
    "A = pd.DataFrame({'x' : x, 'z' : z, 'v' : v})\n",
    "A['log_odds'] = sigmoid(A[['x','z','v']].dot([beta_x,beta_z,beta_v]) + sigma*np.random.normal(0,1,n))\n",
    "A['y'] = [np.random.binomial(1,p) for p in A.log_odds]\n",
    "\n",
    "y, X = dmatrices(formula, A, return_type='dataframe')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"algorithms\"></a>\n",
    "<hr>\n",
    "### Algorithm Setup\n",
    "\n",
    "We begin with a quick function for catching singular matrix errors that we will use to decorate our Newton steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catch_singularity(f):\n",
    "    '''Silences LinAlg Errors and throws a warning instead.'''\n",
    "    \n",
    "    def silencer(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian!')\n",
    "            return args[0]\n",
    "    return silencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"newton\"></a>\n",
    "<hr>\n",
    "### Explanation of a Single Newton Step\n",
    "\n",
    "Recall that Newton's method for maximizing / minimizing a given function $f(\\beta)$ iteratively computes the following estimate:\n",
    "\n",
    "$$\n",
    "\\beta^+ = \\beta - Hf(\\beta)^{-1}\\nabla f(\\beta)\n",
    "$$\n",
    "\n",
    "The Hessian of the log-likelihood for logistic regression is given by:\n",
    "$$\n",
    "Hf(\\beta) = -X^TWX\n",
    "$$\n",
    "and the gradient is:\n",
    "$$\n",
    "\\nabla f(\\beta) = X^T(y-p)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "W := \\text{diag}\\left(p(1-p)\\right)\n",
    "$$\n",
    "and $p$ are the predicted probabilites computed at the current value of $\\beta$.\n",
    "\n",
    "### Connection to Iteratively Reweighted Least Squares (IRLS)\n",
    "*For logistic regression, this step is actually equivalent to computing a weighted least squares estimator at each iteration!*  I.e.,\n",
    "$$\n",
    "\\beta^+ = \\arg\\min_\\beta (z-X\\beta)^TW(z-X\\beta)\n",
    "$$\n",
    "with $W$ as before and the *adjusted response* $z$ is given by\n",
    "$$\n",
    "z := X\\beta + W^{-1}(y-p)\n",
    "$$\n",
    "\n",
    "**Takeaway:** This is fun, but in fact it can be leveraged to derive asymptotics and inferential statistics about the computed MLE $\\beta^*$!\n",
    "\n",
    "### Our implementations\n",
    "Below we implement a single step of Newton's method, and we compute $Hf(\\beta)^{-1}\\nabla f(\\beta)$ using `np.linalg.lstsq(A,b)` to solve the equation $Ax = b$.  Note that this does not require us to compute the actual inverse of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## update\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement Newton's method in a *slightly* different way; this time, at each iteration, we actually compute the full inverse of the Hessian using `np.linalg.inv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conv\"></a>\n",
    "<hr>\n",
    "### Convergence Setup\n",
    "\n",
    "First we implement coefficient convergence; this stopping criterion results in convergence whenever\n",
    "$$\n",
    "\\|\\beta^+ - \\beta\\|_\\infty < \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is a given tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    \n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement log-likelihood convergence.  This stopping criterion results in convergence whenever\n",
    "$$\n",
    "\\left|\\mathcal{L}(x,\\beta^+) - \\mathcal{L}(x,\\beta)\\right| < \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_loglike_convergence(pred_probs, actuals, old_like, tol, iters):\n",
    "    '''Checks whether the log-likelihood has converged.\n",
    "    Returns True if converged, False otherwise.'''\n",
    "    \n",
    "    ## new log-likelihood value\n",
    "    new_like = log_likelihood(pred_probs, actuals)\n",
    "    \n",
    "    return not ((np.abs(old_like-new_like)>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"numerics\"></a>\n",
    "<hr>\n",
    "## Numerical Examples\n",
    "\n",
    "### Standard Newton with Coefficient Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 11\n",
      "Beta : [[  1.79082594e+17]\n",
      " [ -1.08285522e+18]\n",
      " [ -8.55920252e+17]\n",
      " [ -1.75727117e+18]\n",
      " [ -9.70100583e+17]\n",
      " [  5.18123533e+17]]\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "iter_count = 0\n",
    "coefs_converged = False\n",
    "\n",
    "while not coefs_converged:\n",
    "    \n",
    "    beta_old = beta\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    iter_count += 1\n",
    "    \n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Newton with Log-likelihood Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 9\n",
      "Beta : [[-1066.07747757]\n",
      " [-1398.19359417]\n",
      " [    4.88970121]\n",
      " [ -538.16955634]\n",
      " [ 2287.50721864]\n",
      " [  -87.95663379]]\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "beta = np.zeros((len(X.columns),1))\n",
    "old_like, new_like = 1, 0\n",
    "iter_count = 0\n",
    "loglike_converged = False\n",
    "\n",
    "while not loglike_converged:\n",
    "    \n",
    "    p = np.array(sigmoid(X.dot(beta[:,0])), ndmin=2).T\n",
    "    old_like = log_likelihood(p, y)\n",
    "    \n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    p = np.array(sigmoid(X.dot(beta[:,0])), ndmin=2).T\n",
    "    iter_count += 1\n",
    "    \n",
    "    loglike_converged = check_loglike_convergence(p, y, old_like, tol, iter_count)\n",
    "\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Newton with Log-likelihood Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 9\n",
      "Beta : [[-1066.07477481]\n",
      " [-1398.19101457]\n",
      " [    4.88957748]\n",
      " [ -538.16891391]\n",
      " [ 2287.50325827]\n",
      " [  -87.95644978]]\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "beta = np.zeros((len(X.columns),1))\n",
    "old_like, new_like = 1, 0\n",
    "iter_count = 0\n",
    "\n",
    "loglike_converged = False\n",
    "\n",
    "while not loglike_converged:\n",
    "    \n",
    "    p = np.array(sigmoid(X.dot(beta[:,0])), ndmin=2).T\n",
    "    old_like = log_likelihood(p, y)\n",
    "    \n",
    "    beta = alt_newton_step(beta, X, lam=lam)\n",
    "    p = np.array(sigmoid(X.dot(beta[:,0])), ndmin=2).T\n",
    "    iter_count += 1\n",
    "    \n",
    "    loglike_converged = check_loglike_convergence(p, y, old_like, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Newton with Coefficient Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 13\n",
      "Beta : [[  2.92102353e+23]\n",
      " [ -7.43688983e+23]\n",
      " [  1.32775224e+24]\n",
      " [ -4.89685590e+22]\n",
      " [ -2.01242024e+23]\n",
      " [  2.77133562e+21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriswhite1/anaconda3/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Algorithm terminated - singular Hessian!\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "iter_count = 0\n",
    "\n",
    "coefs_converged = False\n",
    "\n",
    "while not coefs_converged:\n",
    "    \n",
    "    beta_old = beta\n",
    "    beta = alt_newton_step(beta, X, lam=lam)\n",
    "    iter_count += 1\n",
    "    \n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## if you want to check against statsmodels\n",
    "# mod = smf.logit(formula, A).fit()\n",
    "# mod.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
